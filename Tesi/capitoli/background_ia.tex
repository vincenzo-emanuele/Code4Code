\chapter{Background sull'Intelligenza Artificiale utilizzata} %\label{1cap:spinta_laterale}
% [titolo ridotto se non ci dovesse stare] {titolo completo}
%

\begin{citazione}
Come anticipato nei precedenti capitoli, è stata posta particolare attenzione alla progettazione e all'implementazione degli strumenti di Intelligenza Artificiale che assistono gli utenti nell'utilizzo della piattaforma. Nel presente capitolo, in primo luogo si motiva l'impiego dell'Intelligenza Artificiale mediante una panoramica generale e ad alto livello sulle problematiche risolvibili con essa. Successivamente vengono analizzate singolarmente le problematiche risolte, introducendo, con esse, gli strumenti di Intelligenza Artificiale impiegati con i relativi fondamenti teorici. Dopo l'infarinatura teorica che serve a comprendere in modo più agevole il lavoro svolto, vengono illustrate le modalità tramite le quali sono stati prelevati i dati sui quali lavorano i moduli di Intelligenza Artificiale sviluppati e vengono fornite specifiche motivazioni riguardanti alcune scelte effettuate in fase implementativa. Il presente capitolo risulta, dunque, essere il più denso di tecnicismi ed il cuore dell'intero lavoro di tesi. La sua stesura ha richiesto uno studio da diverse fonti, indicate nell'apposita sezione, che possono essere consultate per ottenere ulteriori informazioni sui fondamenti teorici e matematici degli strumenti di Intelligenza Artificiale utilizzati. Il capitolo si chiude con la proposta di progettazione di un altro Agente di Intelligenza Artificiale che potrebbe fornire un ulteriore supporto agli utenti della piattaforma.
\end{citazione}
\newpage

\section{Motivazioni all'utilizzo dell'Intelligenza Artificiale} %\label{1sec:scopo}
L'utilizzo dell'Intelligenza Artificiale apre le porte all'implementazione di funzionalità che arricchiscono la piattaforma all'interno della quale vengono integrate. Nel caso della piattaforma \textsc{Code4Code} ci sono due motivazioni principali che portano all'esigenza di utilizzare l'Intelligenza Artificiale:
\begin{itemize}
    \item{\textbf{Suggerimento di Tecnologie Software}: quando un utente si iscrive alla piattaforma potrebbe non avere le idee chiare sulle tecnologie da apprendere. Un apposito modulo di Intelligenza Artificiale lo assisterà consigliandogli tecnologie da imparare sulla base di quelle che già conosce.}
    \item{\textbf{Suggerimento di utenti con cui collaborare}: l'anima della piattaforma è la collaborazione, ma la disponibilità di un utente può variare in base a diversi fattori come il numero di utenti con cui collabora in un certo momento. Mediante un algoritmo di Intelligenza Artificiale è possibile individuare un insieme di utenti con cui risulta più conveniente iniziare una collaborazione evitando lunghe ed inconcludenti ricerche.}
\end{itemize}
Tra queste due problematiche è stata debitamente affrontata, progettata ed implementata solo la prima, mentre della seconda verrà fornita un'idea di progettazione ed implementazione.
\section{Suggerimento di Tecnologie Software} %\label{1sec:scopo}
Quando un utente si iscrive alla piattaforma seleziona le tecnologie Software conosciute, sulla base delle quali gliene vengono suggerite altre da imparare. Una tecnologia Software, in questo specifico caso, può essere un Linguaggio, un Framework o una Libreria, pertanto il problema in questione è stato suddiviso in due parti: 
\begin{itemize}
    \item Suggerimento di Linguaggi a partire da quelli conosciuti, mediante \emph{Association Rules}.
    \item Suggerimento di Librerie e Frameworks a partire dai Linguaggi, dalle Librerie e dai Frameworks conosciuti, mediante \emph{Natural Language Processing}. 
\end{itemize}
L'esigenza di suddividere l'Agente di Intelligenza Artificiale in due micro-agenti è stata dettata dalla eterogeneità dei DataSet disponibili a causa dei quali non è stato possibile pensare ad una soluzione unificata per risolvere il problema. Nei paragrafi successivi verranno trattati gli aspetti teorici ed implementativi delle tecniche di Intelligenza Artificiale utilizzate con riferimenti ai relativi DataSet.
\subsection{Suggerimento di Linguaggi}
Suggerire un Linguaggio ad un utente in base a quelli che conosce, significa proporre all'utente Linguaggi che siano in qualche modo correlati a quelli di sua conoscenza. È sensato considerare due principali tipi di correlazione: \emph{similarità} e \emph{complementarietà}.
\begin{itemize}
    \item Due Linguaggi vengono considerati simili se hanno un certo numero di caratteristiche in comune.
    \item Due Linguaggi vengono considerati complementari se vengono spesso utilizzati insieme nell'ambito di progetti Software.
\end{itemize}
\subsubsection{Similarità tra Linguaggi}
Stabiliti i parametri secondo i quali due Linguaggi sono definibili simili, risulta semplice sviluppare un algoritmo che, presi in input i Linguaggi conosciuti dall'utente, restituisca quelli che hanno più caratteristiche possibili in comune con i Linguaggi input. Le caratteristiche in esame sono: tipo del Linguaggio (Programmazione, Markup, Scripting, Query, Stylesheet [\dots]), paradigmi (Object Oriented, funzionale, procedurale [\dots]) e sistema di tipizzazione (nessuna tipizzazione, tipizzazione forte, tipizzazione debole). 
Supponendo, ad esempio, che il Linguaggio input sia \emph{Java} che è un Linguaggio di Programmazione, il cui paradigma è \emph{Object Oriented}, con sistema di tipizzazione forte, l'algoritmo suggerirà sicuramente \emph{Kotlin} (le cui caratteristiche corrispondono quasi esattamente a quelle di \emph{Java}) in misura maggiore rispetto ad un Linguaggio di Markup come ad esempio \emph{HTML} o ad un Linguaggio di Scripting come ad esempio \emph{Lua}. 
La metrica adottata per quantificare quanto un Linguaggio sia effettivamente consigliato ad un utente è esattamente il numero di caratteristiche che il Linguaggio input e gli altri Linguaggi hanno in comune. 
L'algoritmo non restituisce un unico Linguaggio, bensì un insieme di Linguaggi aventi un certo numero di caratteristiche in comune con almeno uno dei Linguaggi input. Il numero massimo di Linguaggi da consigliare all'utente è stato scelto in seguito ad alcuni test per individuare la soluzione migliore: se la dimensione \emph{n} dell'insieme dei Linguaggi restituiti dall'algoritmo supera le 5 unità, si considerano solo i primi \(\frac{n}{2}\) Linguaggi più simili, in caso contrario si considerano tutti i Linguaggi restituiti. \\
Dal momento che i dati da esaminare riguardano le caratteristiche dei vari Linguaggi, è stato ottenuto un semplice DataSet prelevando i dati da siti Web come \emph{Wikipedia}. L'algoritmo vero e proprio è stato sviluppato in \emph{Java} senza l'ausilio di particolari Framework o Librerie. \\
L'approccio appena descritto non rappresenta una vera e propria applicazione di Intelligenza Artificiale, ma solo una base esemplificativa da cui partire per poter stabilire, in maniera intuitiva, la similarità tra due Linguaggi. L'approccio basato sulla complementarietà, descritto di seguito, costituisce un esempio di applicazione dell'Intelligenza Artificiale sicuramente più articolato rispetto a quello appena presentato.
\subsubsection{Complementarietà tra Linguaggi}
Focalizzarsi sulla complementarietà tra i Linguaggi consente, a chi vuole imparare, di acquisire competenze a 360 gradi.
 Come anticipato in precedenza, due Linguaggi sono considerati complementari se vengono spesso utilizzati insieme nell'ambito di progetti Software; per indagare sulla complementarietà tra i Linguaggi, è necessario disporre sia di un insieme di progetti Software di dimensione sufficientemente elevata, al fine di poter effettuare considerazioni reali sull'associazione tra i Linguaggi, che di una tecnica per interpretare e dare un senso ai dati relativi ai progetti. Al fine di ottenere un insieme considerevole di progetti Software è stato utilizzato \emph{GitHub} ed in particolar modo è stato fatto riferimento alle Repositories pubbliche.
\emph{GitHub} mette a disposizione \emph{API} pubbliche e gratuite che assolvono a diversi scopi, tra cui cercare i progetti ed ottenere informazioni relative ad essi. Tuttavia, per semplificare il lavoro e migliorare la qualità del DataSet è stato utilizzato un tool online denominato \emph{SEART GitHub Search Engine} \cite{Dabic:msr2021data} che consente, mediante un'interfaccia molto intuitiva, di filtrare i risultati secondo determinate caratteristiche della Repository come: numero di commits, numero di stars, numero di releases, range di data. 
Effettuare un filtraggio del genere consente di evitare Repositories non rilevanti (Repositories di prova, piccoli progetti di esempio, esercizi didattici) focalizzando l'attenzione solo su quelle che possano in qualche modo essere rilevanti ai fini della trattazione del problema in questione. 
I parametri utilizzati per filtrare le Repositories sono stati scelti in maniera intuitiva e sono stati effettuati dei test per confrontarli tra loro. Il risultato migliore è stato ottenuto prelevando tutte le Repositories con almeno 50 commits e 100 stars. Il Tool sopracitato ha prodotto un file output in formato \emph{JSON}, di cui è stato effettuato un parsing utilizzando uno script scritto in \emph{Python} tramite il quale sono state estratte le informazioni rilevanti per la risoluzione del problema in questione, ossia i Linguaggi impiegati nell'ambito delle Repositories con la relativa quantità di byte di codice utilizzato. È stato, dunque, creato un file in cui su ogni riga è riportata la lista di Linguaggi utilizzati in una determinata Repository, ciascuno dei quali è seguito dalla quantità di byte di codice scritto in quel Linguaggio. Ogni riga è, dunque, associata ad una specifica Repository ed in totale sono stati estratti i dati relativi a esattamente 51135 Repositories. 
Il file appena descritto funge da DataSet per l'algoritmo che cerca la correlazione tra i Linguaggi basandosi sulla complementarietà tra gli stessi. 
I dati vengono interpretati mediante le \emph{Association Rules} \cite{AssociationRules}, un metodo che serve ad estrarre relazioni nascoste da una grande quantità di dati. Nel caso in esame, la relazione da ricercare nei dati è la presenza simultanea di Linguaggi nella stessa Repository; le Association Rules fanno proprio questo tipo di lavoro considerando tre misure di probabilità denominate \emph{Support, Confidence e Lift}.
\begin{itemize}
    \item Il \textbf{Support} è una misura che indica la frequenza con cui un determinato Linguaggio compare nei dati in esame. Il Support di un linguaggio L in un DataSet contenente N Repositories si calcola nel seguente modo: 
        \begin{equation}    
            Support(L)=\frac{Freq(L)}{N}
        \end{equation}    
    \item La \textbf{Confidence} è una misura che indica la frequenza con cui un Linguaggio L\textsubscript2 è presente nelle Repositories che contengono un Linguaggio L\textsubscript1. La formula per calcolare la Confidence è la seguente:
        \begin{equation}
            Confidence(L_1 \rightarrow L_2)=\frac{Support(L_1 \cap L_2)}{Support(L_1)}
        \end{equation}
    \item Il \textbf{Lift} è una misura simile alla \emph{Confidence}, in quanto indica la frequenza con cui un Linguaggio L\textsubscript2 è presente nelle Repositories che contengono un Linguaggio L\textsubscript1, tuttavia tiene conto anche della popolarità del Linguaggio L\textsubscript2. Dal punto di vista matematico, si calcola nel seguente modo:
        \begin{equation}
            Lift(L_2)=\frac{Support(L_1 \cap L_2)}{Support(L_1) \times Support(L_2)}
        \end{equation}
\end{itemize}
L'algoritmo Apriori sfrutta queste tre misure di probabilità al fine di generare le Association Rules. Se si considera un sottoinsieme di k Linguaggi del DataSet sarà possibile generare $2^k-2$ Association Rules, ciascuna delle quali avrà la seguente forma:
\begin{equation}
    \{L_1, L_2, \dots, L_i\} \rightarrow \{L_{i+1}, \dots, L_k\}
\end{equation}
Ad ogni Association Rule sono associate la Confidence e il Lift relativi ai Linguaggi $\{L_{i+1}, \dots, L_k\}$ (tale termine della Regola è detto \emph{conseguente}) considerando le Repository contenenti i Linguaggi $\{L_1, L_2, \dots, L_i\}$ (tale termine della Regola è detto \emph{antecedente}), e ad ogni gruppo di Association Rules relativo ad un sottoinsieme di k Linguaggi è associato il Support dei Linguaggi stessi.\\
Per generare la Association Rules, l'algoritmo Apriori deve seguire una serie di passi in maniera iterativa. Per comprendere l'algoritmo è necessario conoscere il concetto di \emph{k-itemset}: un \emph{k-itemset} è un insieme di \emph{k} elementi che vengono associati tra loro; nel caso in esame un \emph{k-itemset} è un insieme di \emph{k} Linguaggi. L'algoritmo, a partire dal DataSet dei Linguaggi delle Repositories, genera iterativamente gli \emph{itemset} partendo da un \emph{1-itemset} fino ad arrivare ad una condizione di terminazione, per cui la generazione arriverà fino ad un \emph{k-itemset}. Ad ogni \emph{itemset} generato, vengono esclusi gli item che non raggiungono la soglia \emph{s} di Support minima che viene presa in input dall'algoritmo. Per generare un \emph{1-itemset}, viene seguita una procedura molto semplice: si conteggia ogni singolo Linguaggio ottenendo il numero di volte in cui compare nelle Repositories, si escludono i Linguaggi che compaiono nelle Repositories con una percentuale inferiore al Support \emph{s} indicato. Tale lista di Linguaggi rappresenta l'\emph{1-itemset} e sarà la base per costruire il \emph{2-itemset}. Infatti, per la generazione del \emph{2-itemset} vengono considerati i Linguaggi ottenuti dall'\emph{1-itemset} e ad ognuno di essi viene associato ogni altro Linguaggio possibile, ottenendo un \emph{itemset} fatto da coppie (ossia un \emph{2-itemset}). Dall'\emph{itemset} così ottenuto vengono eliminati gli elementi (le coppie di Linguaggi) il cui Support è al di sotto della soglia \emph{s} citata in precedenza. Si procede iterativamente nel modo descritto, tuttavia dal \emph{3-itemset} in poi diventa evidente una specifica proprietà dell'algoritmo: ogni possibile sottoinsieme di un \emph{k-itemset} deve essere incluso in uno dei precedenti \emph{itemset}. In altri termini, ogni sottoinsieme di un \emph{k-itemset} deve avere un Support superiore alla soglia \emph{s}. Tale proprietà dell'algoritmo porta alla condizione di terminazione in quanto l'algoritmo si ferma nel momento in cui non è più possibile generare un \emph{itemset} di cui tutti i sottoinsiemi appartengono ad un \emph{itemset} generato precedentemente. Arrivati all'ultimo \emph{k-itemset}, l'algoritmo riesce a generare tutte le possibili Association Rules relative ai Linguaggi, andando a considerare tutte le possibili combinazioni di Linguaggi nel termine antecedente e nel termine conseguente delle regole. Verranno scartate le regole aventi una Confidence o un Lift al di sotto delle soglie prese in input dall'algoritmo.\\
L'algoritmo Apriori non è stato implementato da zero, ma è stata utilizzata una sua implementazione in \emph{Python} denominata \emph{Apyori} \cite{Apyori} che consente di eseguire l'algoritmo in maniera agevole a partire dal DataSet di Repositories descritto in precedenza e di impostare i parametri di funzionamento dell'algoritmo come la soglia di Support, Confidence e Lift minime dei Linguaggi che l'algoritmo deve considerare e la dimensione massima dei sottoinsiemi che devono essere usati dall'algoritmo per generare le Association Rules. A tal proposito, i parametri che sono stati scelti riguardano la soglia di Support e Confidence poste rispettivamente a 0.0045 e 0.10.\\
Quanto descritto fino ad adesso non dipende dall'input dell'utente e fa, dunque, parte di un lavoro che viene svolto offline e che funge da base per l'algoritmo vero e proprio che a partire dai Linguaggi conosciuti dall'utente ne suggerisce altri. Tale algoritmo, infatti, preleva tutte le Association Rules che come antecedente contengono i Linguaggi input e considera i Linguaggi contenuti nel termine conseguente; la lista di questi Linguaggi verrà chiamata \emph{Lista di Linguaggi candidati}. Ad ogni Linguaggio facente parte della \emph{Lista di Linguaggi candidati}, l'algoritmo associa la Confidence della Regola a cui appartiene (in caso di Linguaggi duplicati considera quello appartenente alla Regola con Confidence maggiore) e consiglia alcuni di questi Linguaggi agli utenti. Non vengono consigliati tutti i Linguaggi candidati per due motivi principali: 
\begin{itemize}
    \item Il primo motivo riguarda aspetti puramente legati all'usabilità della piattaforma, per cui non si consigliano troppi Linguaggi all'utente per evitare di confonderlo. A tale scopo dopo aver ottenuto la lista dei Linguaggi candidati, l'algoritmo controlla la sua dimensione \emph{n} e se questa è inferiore o uguale a 10, non scarta alcun Linguaggio, se è compresa tra 10 e 19, conserva solo gli \emph{$\frac{n}{2}$} Linguaggi a cui è associata la Confidence più elevata, se supera 20 vengono conservati solo gli \emph{$\frac{n}{4}$} Linguaggi con la Confidence più elevata. 
    \item Il secondo motivo riguarda la presenza di Linguaggi nelle Repositories che pur essendo molto frequenti, non risultano rilevanti. Ad esempio, in molti progetti sono presenti brevi script scritti in Bash utilizzati per motivi riguardanti la configurazione e l'esecuzione del progetto; non sarebbe, dunque, corretto consigliare Bash unicamente perché molto usato insieme ad un determinato Linguaggio input.
    Risulta, dunque, necessario svolgere un lavoro di tipo statistico su ogni Linguaggio candidato per determinare se, nelle Repositories in cui viene utilizzato insieme ad almeno uno dei Linguaggi input dell'utente, ha una percentuale di utilizzo almeno pari ad una determinata soglia. La percentuale di utilizzo di un Linguaggio in una Repository può essere facilmente ricavata a partire dalla quantità di byte di codice scritto in quel Linguaggio; ai fini dell'algoritmo viene effettuata una media delle percentuali di codice scritto nel Linguaggio candidato considerato: verranno scartati tutti i Linguaggi la cui media di percentuale di codice impiegato nelle Repositories risulta essere inferiore al 3.5\% (tale soglia è stata scelta dopo aver effettuato diversi test e confrontato tra loro i risultati).
\end{itemize}
\subsection{Suggerimento di Frameworks e Librerie: Natural Language Processing}
Suggerire Frameworks e Librerie ad un utente a partire dai Linguaggi, dai Frameworks e dalle Librerie che conosce, si è rivelato essere un lavoro meno agevole del precedente, in quanto le \emph{API} di \emph{GitHub} consentono di ricavare i Linguaggi adoperati in una Repository, ma non i Frameworks e le Librerie. È stata, dunque, adottata una tecnica di Intelligenza Artificiale diversa dalla precedente, al fine di sfruttare al meglio il DataSet in possesso; tale DataSet contiene:
\begin{itemize}
    \item Descrizioni in lingua inglese di Linguaggi, Framework e Librerie estratti tramite uno scraper scritto in \emph{JavaScript} dalla pagina di \emph{GitHub} contenente i Topic relativi alle Repositories. 
    \item Lista di tag relativi a diverse Repositories estratti da \emph{GitHub} tramite uno scraper appositamente scritto in \emph{JavaScript}. A differenza del caso dei Linguaggi, tuttavia, i tag sono inseriti dagli utenti e non sempre alla stessa Libreria/Framework corrisponde la stessa stringa, in quanto spesso vengono utilizzati nomi alternativi ed abbreviazioni (ciò non accade nel caso dei Linguaggi in quanto vengono automaticamente impostati da \emph{GitHub}). 
\end{itemize}
Vista l'irregolarità e l'informalità del DataSet in esame, si è deciso di ricorrere a tecniche di \emph{Natural Language Processing} al fine di interpretare e dare un senso ai dati ed estrarre informazioni utili sulla correlazione tra i Frameworks, sulla correlazione tra le Librerie e sulla correlazione tra Frameworks e Librerie.\\
L'algoritmo di NLP utilizzato è \emph{Word2Vec} \cite{Word2Vec} che è basato su una rete neurale con un singolo livello nascosto il cui scopo è quello di calcolare un insieme di vettori al fine di valutare la similarità tra le parole facenti parte del DataSet. In particolare, esistono due versioni distinte del modello \emph{Word2Vec}:
\begin{itemize}
    \item Il modello \emph{Skip-grams} \cite{SkipGram}, che ha come obiettivo quello di calcolare la distribuzione di probabilità delle parole che si trovano intorno ad una determinata parola input. Per la risoluzione del problema in questione è stato utilizzato questo tipo di modello.
    \item Il modello \emph{Continuous-bag-of-words}, che ha come obiettivo quello di predire la "parola centrale" dato un certo numero di parole di contesto. 
\end{itemize}
Quando si parla di \emph{Word2Vec} è necessario dettagliare il concetto di DataSet mediante termini specifici: in primo luogo l'unità di base del DataSet è il \emph{documento}, definito come testo di qualsiasi tipo (una breve descrizione, un paper, un libro...) e che nel caso in questione risulta essere la descrizione di una Libreria/Framework oppure l'insieme di tag relativi ad una singola Repository. Un insieme di \emph{documenti} forma un \emph{corpus}, che funge da input per l'algoritmo e dal quale si estraggono le parole al fine di costruire un dizionario.\\
Come anticipato, l'algoritmo \emph{Word2Vec} fa uso di una rete neurale, nella quale è possibile individuare:
\begin{itemize}
    \item Un livello input che trasforma le parole in vettori: in primo luogo l'algoritmo crea un dizionario contenente tutte le \emph{n} parole del \emph{corpus} e alla parola i-esima associa un vettore di dimensione \emph{$1\times n$} composto da valori tutti pari a 0 e da un 1 in posizione i-esima; tali vettori sono chiamati \emph{one-hot vector}. 
    \item Un livello nascosto che contiene \emph{n} vettori, ciascuno di dimensione \emph{m}; la scelta di \emph{m} è, generalmente, problem specific e rappresenta il numero di \emph{features} delle parole nel dizionario considerato. Lo scopo della fase di addestramento è quello di apprendere i valori delle componenti dei vettori dello strato nascosto della rete, in quanto rappresentano le caratteristiche vere e proprie delle parole del dizionario; tali vettori sono denominati \emph{embeddings} e il livello nascosto della rete contiene una matrice \emph{$n \times m$} di tutti gli \emph{embeddings} relativi alle parole del dizionario.
    \item Un livello output nel quale viene effettuato il prodotto tra l'\emph{embedding} della parola input e la matrice che contiene tutti gli \emph{embeddings} trasposta, e successivamente al risultato viene applicata la funzione di regressione Softmax, ottenendo un vettore contenente la probabilità che ogni parola del dizionario si trovi nel contesto della parola input.
\end{itemize} 
L'addestramento della rete neurale consiste in diverse fasi: in primo luogo vi è la definizione di un valore intero detto \emph{window size} che rappresenta il numero di parole da considerare a sinistra e a destra di una parola \emph{w} all'interno di una frase del \emph{corpus}. Tali parole saranno considerate appartenenti allo stesso contesto di \emph{w} e sono dette \emph{parole di contesto}. Vengono, a questo punto, costruiti dei campioni di addestramento ossia delle coppie \emph{(w, x)} dove \emph{w} è una parola del dizionario e \emph{x} è una sua parola di contesto; generate tutte queste coppie, viene data in input alla rete neurale la parola \emph{w}, ottenendo, in questo modo, un vettore output \emph{v} contenente le probabilità che ogni parola del dizionario si trovi nel contesto di \emph{w}. A questo punto si confronta il vettore \emph{v} con il vettore \emph{x} (che essendo un \emph{one-hot vector} conterrà un 1 nella posizione relativa alla parola di contesto) e si aggiornano gli \emph{embeddings} del livello nascosto della rete utilizzando il metodo della discesa del gradiente. Tale tipo di addestramento fa sì che i pesi del livello intermedio vadano a modificarsi gradualmente sulla base delle coppie di parole vicine che vengono prelevate dal \emph{corpus}: al termine dell'addestramento, le parole che vengono considerate appartenenti allo stesso contesto, avranno i relativi \emph{embeddings} simili. Dal momento che un \emph{embedding} non è altro che un vettore, il risultato dell'addestramento può essere immaginato in uno spazio a \emph{m} dimensioni in cui ogni parola è rappresentata da un punto le cui coordinate sono descritte dal corrispondente \emph{embedding}, per cui parole usate in contesti simili, e quindi con \emph{embeddings} simili si tradurranno in punti molto vicini tra loro nello spazio a \emph{m} dimensioni. In questo modo, la rete neurale riesce ad apprendere il contesto in cui un Framework o una Libreria vengono usati e ad affiancare tra loro Frameworks e Librerie in base al contesto appreso.\\
L'algoritmo appena descritto non è stato implementato da zero, ma è stata utilizzata una Libreria scritta in \emph{Python} denominata \emph{Gensim} \cite{rehurek_lrec} che offre l'implementazione dell'algoritmo \emph{Word2Vec} con la possibilità di impostare i parametri per l'addestramento della rete neurale; a tale scopo è stata impostata la dimensione dello spazio vettoriale a 200 e il numero di epoche della rete a 50. Prima di sottoporre il \emph{corpus} all'algoritmo, è stata effettuata una fase di \emph{preprocessing} che ha rimosso le \emph{stopwords} dai \emph{documenti} del \emph{corpus}, in particolare dalle descrizioni dei Frameworks e delle Librerie. Il lavoro descritto fino a questo momento viene svolto \emph{una tantum} in quanto funge da addestramento per la rete neurale, al seguito del quale viene salvato il modello addestrato in maniera persistente. 
Una volta salvato il modello addestrato, è possibile testarlo su diversi input al fine di stabilire le parole più simili, in termini di contesto, alla parola input. Ponendo nuovamente l'attenzione sulla piattaforma \textsc{Code4Code}, la parola input sarà un Linguaggio, un Framework o una Libreria, così come dalle parole output verranno selezionati solo i Frameworks e le Librerie. In particolare verranno selezionati solo quelli la cui probabilità di trovarsi nel contesto della parola input è superiore al 50\%. L'utente, dunque, seleziona i Linguaggi e i Framework che conosce, e la rete neurale processerà ognuna di queste tecnologie per produrre un insieme di Frameworks e Librerie che gli consiglierà.
\section{Suggerimento di utenti con cui collaborare}
La questione relativa al suggerimento di utenti con cui collaborare nell'ambito della piattaforma, non è stata interamente progettata ed implementata com'è evidentemente accaduto nel caso della questione del suggerimento delle tecnologie Software, tuttavia è possibile fornire un accenno di progettazione in merito. È chiaro che due utenti possano collaborare se uno conosce tecnologie che interessano all'altro, tuttavia questo non è l'unico fattore in gioco che determina la possibilità di due utenti di collaborare; si possono considerare, in astratto, un insieme di fattori in gioco e si può immaginare di disporre di un algoritmo che, a partire da un insieme di utenti, tenendo conto di tali fattori consiglia all'utente un insieme di partner con cui collaborare. L'algoritmo in questione dovrebbe massimizzare (o minimizzare) una funzione di fitness che rispecchia un qualche tipo di combinazione matematica dei parametri. I parametri da considerare dipendono strettamente dalla piattaforma; alcuni di questi riguardano il numero di partner di un determinato utente e le recensioni acquisite nell'insegnamento di una determinata tecnologia. Tra i diversi algoritmi utilizzabili, sono stati valutati gli algoritmi genetici, ossia algoritmi di ricerca che si ispirano alla genetica e si prestano bene alla risoluzione di questo problema sia per via della tipologia della funzione obiettivo che è basata sull'ottimizzazione di diversi parametri, che per la struttura del problema che sarebbe facilmente traslabile sulle strutture di un algoritmo genetico. Tale tipo di algoritmo (che sarebbe più corretto definire meta-euristica) effettua una ricerca "portando avanti" un insieme di soluzioni anziché una singola e conserva quelle considerate più promettenti dal punto di vista della funzione di fitness. Come anticipato in precedenza, gli algoritmi genetici hanno diverse analogie con la genetica, in particolare le soluzioni che vengono "portate avanti" sono definite "individui" e le componenti che formano tali soluzioni sono i "geni" degli individui. Per mostrare la similarità tra le strutture dell'algoritmo genetico e quelle del problema in questione, è possibile effettuare un parallelismo tra i due contesti: un individuo è un gruppo di utenti candidati per essere consigliati e un gene è un singolo utente. La trattazione appena effettuata ha lo scopo di dare una semplice intuizione su quella che può essere l'idea alla base della risoluzione di tale problema. Se si volesse approfondire anche soltanto il discorso degli algoritmi genetici, sarebbe doveroso approfondire diversi aspetti che sono stati tralasciati, come ad esempio gli operatori genetici che vengono utilizzati per far mutare ed evolvere le soluzioni. 
\newpage
