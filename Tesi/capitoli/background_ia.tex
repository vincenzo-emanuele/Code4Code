\chapter{Background sull'Intelligenza Artificiale utilizzata} %\label{1cap:spinta_laterale}
% [titolo ridotto se non ci dovesse stare] {titolo completo}
%

\begin{citazione}
Come anticipato nei precedenti capitoli, è stata posta particolare attenzione alla progettazione e all'implementazione degli strumenti di Intelligenza Artificiale che assistono gli utenti nell'utilizzo della piattaforma. Il presente capitolo risulta, dunque, essere il cuore del lavoro di tesi in quanto tratta e approfondisce diversi aspetti relativi all'Intelligenza Artificiale. 
\end{citazione}
\newpage

\section{Motivazioni all'utilizzo dell'Intelligenza Artificiale} %\label{1sec:scopo}
L'utilizzo dell'Intelligenza Artificiale apre le porte all'implementazione di funzionalità che arricchiscono la piattaforma all'interno della quale vengono integrate. Nel caso della piattaforma \textsc{Code4Code} ci sono due motivazioni principali che portano all'esigenza di utilizzare l'Intelligenza Artificiale:
\begin{itemize}
    \item{\textbf{Suggerimento di Tecnologie Software}: Quando un utente si iscrive alla piattaforma potrebbe non avere le idee chiare sulle Tecnologie da apprendere: un apposito modulo di Intelligenza Artificiale lo assisterà consigliandogli Tecnologie da imparare sulla base di quelle che già conosce.}
    \item{\textbf{Suggerimento di utenti con cui collaborare}: L'anima della piattaforma è la collaborazione, ma la disponibilità di un utente può variare in base a diversi fattori come il numero di utenti con cui collabora in un certo momento. Mediante un algoritmo di Intelligenza Artificiale è possibile individuare un insieme di utenti con cui risulta più conveniente iniziare una collaborazione evitando lunghe ed inconcludenti ricerche nel momento in cui si è in procinto di iniziare una collaborazione.}
\end{itemize}
Tra queste due problematiche è stata debitamente affrontata, progettata ed implementata solo la prima, mentre della seconda verrà fornita solo un'idea di progettazione ed implementazione.
\section{Suggerimento di Tecnologie Software} %\label{1sec:scopo}
Quando un utente si iscrive alla piattaforma seleziona le Tecnologie Software conosciute, sulla base delle quali gliene vengono suggerite altre da imparare. Una Tecnologia Software, in questo specifico caso, può essere un Linguaggio \footnote{Nel corso della trattazione, a scopo esemplificativo, si utilizzerà il termine \emph{Linguaggio} per riferirsi, in generale, alla classe dei Linguaggi formali che comprende Linguaggi di Programmazione, Markup, Scripting, Stylesheet [\dots]}, un Framework o una Libreria, pertanto il problema in questione è stato suddiviso in due parti: 
\begin{itemize}
    \item Suggerimento di Linguaggi a partire da quelli conosciuti, mediante \emph{Association Rules}.
    \item Suggerimento di Librerie e Frameworks a partire dai Linguaggi, dalle Librerie e dai Frameworks conosciuti, mediante \emph{Natural Language Processing}. 
\end{itemize}
L'esigenza di suddividere l'Agente di Intelligenza Artificiale in due micro-agenti è stata dettata dalla eterogeneità dei DataSet disponibili a causa dei quali non è stato possibile pensare ad una soluzione unificata per risolvere il problema. Nei paragrafi successivi verranno trattati gli aspetti teorici ed implementativi delle tecniche di Intelligenza Artificiale utilizzate con riferimenti ai relativi DataSet.
\subsection{Suggerimento di Linguaggi}
Suggerire un Linguaggio di Programmazione ad un utente in base a quelli che conosce, significa proporre all'utente Linguaggi che siano in qualche modo correlati a quelli di sua conoscenza. È sensato considerare due principali tipi di correlazione: \emph{similarità} e \emph{complementarietà}.
\begin{itemize}
    \item Due Linguaggi vengono considerati simili se hanno un certo numero di caratteristiche in comune.
    \item Due Linguaggi vengono considerati complementari se vengono spesso utilizzati insieme nell'ambito di Progetti Software.
\end{itemize}
\subsubsection{Similarità tra Linguaggi}
Stabiliti i parametri secondo i quali due Linguaggi sono definibili simili, risulta semplice sviluppare un Algoritmo che, presi in input i Linguaggi conosciuti dall'utente, restituisca quelli che hanno più caratteristiche possibili in comune con i Linguaggi input. Le caratteristiche in esame sono: tipo del Linguaggio (Programmazione, Markup, Scripting, Query, Stylesheet [\dots]), paradigmi (Object Oriented, funzionale, procedurale [\dots]) e sistema di tipizzazione (nessuna tipizzazione, tipizzazione forte, tipizzazione debole). 
Supponendo, ad esempio, che il Linguaggio input sia \emph{Java} che è un Linguaggio di Programmazione, il cui paradigma è \emph{Object Oriented}, con sistema di tipizzazione forte, l'Algoritmo suggerirà sicuramente \emph{Kotlin} (le cui caratteristiche corrispondono quasi esattamente a quelle di \emph{Java}) in misura maggiore rispetto ad un Linguaggio di Markup come ad esempio \emph{HTML} o ad un Linguaggio di Scripting come ad esempio \emph{Lua}. 
La metrica adottata per quantificare quanto un Linguaggio sia effettivamente consigliato ad un utente è esattamente il numero di caratteristiche che il Linguaggio input e gli altri Linguaggi hanno in comune. 
L'Algoritmo non restituisce un unico Linguaggio, bensì un insieme di Linguaggi aventi un certo numero di caratteristiche in comune con almeno uno dei Linguaggi input. Il numero massimo di Linguaggi da consigliare all'utente è stato scelto in seguito ad alcuni test per individuare la soluzione migliore: se la dimensione \emph{n} dell'insieme dei Linguaggi restituiti dall'Algoritmo supera le 5 unità, si considerano solo i primi \(\frac{n}{2}\) Linguaggi più simili, in caso contrario si considerano tutti i Linguaggi restituiti. \\
Dal momento che i dati da esaminare riguardano le caratteristiche dei vari Linguaggi, è stato ottenuto un semplice DataSet prelevando i dati da siti Web come \emph{Wikipedia}. L'Algoritmo vero e proprio è stato sviluppato in \emph{Java} senza l'ausilio di particolari Framework o Librerie. \\
L'approccio appena descritto non rappresenta una vera e propria applicazione di Intelligenza Artificiale, ma solo una base esemplificativa da cui partire per poter stabilire, in maniera intuitiva, la similarità tra due Linguaggi. L'approccio basato sulla complementarietà, descritto di seguito, costituisce un esempio di applicazione dell'Intelligenza Artificiale sicuramente più articolato rispetto a quello appena presentato.
\subsubsection{Complementarietà tra Linguaggi}
Focalizzarsi sulla complementarietà tra i Linguaggi consente, a chi vuole imparare, di acquisire competenze a 360 gradi.
 Come anticipato in precedenza, due Linguaggi sono considerati complementari se vengono spesso utilizzati insieme nell'ambito di Progetti Software; per indagare sulla complementarietà tra i Linguaggi, è necessario disporre sia di un insieme di Progetti Software di dimensione sufficientemente elevata, al fine di poter effettuare considerazioni reali sull'associazione tra i Linguaggi, che di una tecnica per interpretare e dare un senso ai dati relativi ai progetti. Al fine di ottenere un insieme considerevole di Progetti Software è stato utilizzato \emph{GitHub} ed in particolar modo è stato fatto riferimento alle Repositories pubbliche.
\emph{GitHub} mette a disposizione API pubbliche e gratuite che assolvono a diversi scopi, tra cui cercare i progetti ed ottenere informazioni relative ad essi. Tuttavia, per semplificare il lavoro e migliorare la qualità del DataSet è stato utilizzato un tool online denominato \emph{SEART GitHub Search Engine} \cite{Dabic:msr2021data} che consente, mediante un'interfaccia molto intuitiva, di filtrare i risultati secondo determinate caratteristiche della Repository come: numero di commits, numero di stars, numero di releases, range di data. 
Effettuare un filtraggio del genere consente di evitare Repositories non rilevanti (Repositories di prova, piccoli progetti di esempio, esercizi didattici) focalizzando l'attenzione solo su quelle che possano in qualche modo essere rilevanti ai fini della trattazione del problema in questione. 
I parametri utilizzati per filtrare le Repositories sono stati scelti in maniera intuitiva e sono stati effettuati dei test per confrontarli tra loro. Il risultato migliore è stato ottenuto prelevando tutte le Repositories con almeno 50 commit e 100 stars. Il Tool sopracitato ha prodotto un file output in formato \emph{JSON}, del quale è stato effettuato un parsing utilizzando uno script scritto in \emph{Python} tramite il quale sono state estratte le informazioni rilevanti per la risoluzione del problema in questione, ossia i Linguaggi impiegati nell'ambito delle Repositories con la relativa quantità di byte di codice utilizzato. È stato, dunque, creato un file in cui su ogni riga è riportata la lista di Linguaggi utilizzati in una determinata Repository, ciascuno dei quali è seguito dalla quantità di byte di codice scritto in quel Linguaggio. Ogni riga è, dunque, associata ad una specifica Repository ed in totale sono stati estratti i dati relativi a esattamente 51135 Repositories. \\
Il file appena descritto funge da DataSet per l'Algoritmo che cerca la correlazione tra i Linguaggi basandosi sulla complementarietà tra gli stessi. 
I dati vengono interpretati mediante le \emph{Association Rules} \cite{AssociationRules}, un metodo che serve ad estrarre relazioni nascoste da una grande quantità di dati. Nel caso in esame, la relazione da ricercare nei dati è la presenza simultanea di Linguaggi nella stessa Repository; le Association Rules fanno proprio questo tipo di lavoro considerando tre misure di probabilità denominate \emph{Support, Confidence e Lift}.
\begin{itemize}
    \item Il \textbf{Support} è una misura che indica la frequenza in cui un determinato Linguaggio compare nei dati in esame. Il Support di un linguaggio L in un DataSet contenente N Repositories si calcola nel seguente modo: 
        \begin{equation}    
            Support(L)=\frac{Freq(L)}{N}
        \end{equation}    
    \item La \textbf{Confidence} è una misura che indica la frequenza con cui un Linguaggio L\textsubscript2 è presente nelle Repositories che contengono un Linguaggio L\textsubscript1. La formula per calcolare la Confidence è la seguente:
        \begin{equation}
            Confidence(L_1 \rightarrow L_2)=\frac{Support(L_1 \cap L_2)}{Support(L_1)}
        \end{equation}
    \item Il \textbf{Lift} è una misura simile alla \emph{Confidence}, in quanto indica la frequenza con cui un Linguaggio L\textsubscript2 è presente nelle Repositories che contengono un Linguaggio L\textsubscript1, tuttavia tiene conto anche della popolarità del Linguaggio L\textsubscript2. Dal punto di vista matematico, si calcola nel seguente modo:
        \begin{equation}
            Lift(L_2)=\frac{Support(L_1 \cap L_2)}{Support(L_1) \times Support(L_2)}
        \end{equation}
\end{itemize}
L'algoritmo Apriori sfrutta queste tre misure di probabilità al fine di generare le Association Rules. Tralasciando i dettagli implementativi dell'Algoritmo Aprori (che possono essere approfonditi alla fonte indicata \cite{AssociationRules}), se si considera un sottoinsieme di k Linguaggi del DataSet sarà possibile generare $2^k-2$ Association Rules, ciascuna delle quali avrà la seguente forma:
\begin{equation}
    \{L_1, L_2, \dots, L_i\} \rightarrow \{L_{i+1}, \dots, L_k\}
\end{equation}
Ad ogni Association Rule sono associate la Confidence e il Lift relativi ai Linguaggi $\{L_{i+1}, \dots, L_k\}$ (tale termine della Regola è detto \emph{conseguente}) considerando le Repository contenenti i Linguaggi $\{L_1, L_2, \dots, L_i\}$ (tale termine della Regola è detto \emph{antecedente}), e ad ogni gruppo di Association Rules relativo ad un sottoinsieme di k Linguaggi è associato il Support dei Linguaggi stessi.
L'Algoritmo Apriori non è stato implementato da zero, ma è stata utilizzata una sua implementazione in Python denominata \emph{Apyori} \cite{Apyori} che consente di eseguire l'algoritmo in maniera agevole a partire dal DataSet di Repositories descritto in precedenza e di impostare i parametri di funzionamento dell'Algoritmo come la soglia di Support, Confidence e Lift minime dei Linguaggi che l'Algoritmo deve considerare e la dimensione massima dei sottoinsiemi che devono essere usati dall'Algoritmo per generare le Association Rules. A tal proposito, i parametri che sono stati scelti riguardano la soglia di Support e Confidence poste rispettivamente a 0.0045 e 0.10.\\
Quanto descritto fino ad adesso non dipende dall'input dell'utente e fa, dunque, parte di un lavoro che viene svolto offline e che funge da base per l'Algoritmo vero e proprio che a partire dai Linguaggi conosciuti dall'utente ne suggerisce altri. Tale Algoritmo, infatti, preleva tutte le Association Rules che come antecedente contengono i Linguaggi input e considera i Linguaggi contenuti nel termine conseguente; la lista di questi Linguaggi verrà chiamata \emph{Lista di Linguaggi candidati}. Ad ogni Linguaggio facente parte della \emph{Lista di Linguaggi candidati}, l'Algoritmo associa la Confidence della Regola a cui appartiene (in caso di Linguaggi duplicati considera quello appartenente alla Regola con Confidence maggiore) e consiglia alcuni di questi Linguaggi agli utenti. Non vengono consigliati tutti i Linguaggi candidati per due motivi principali: 
\begin{itemize}
    \item Il primo motivo riguarda aspetti puramente legati all'usabilità della piattaforma, per cui non si consigliano troppi Linguaggi all'utente per evitare di confonderlo. A tale scopo dopo aver ottenuto la lista dei Linguaggi candidati, l'Algoritmo controlla la sua dimensione \emph{n} e se questa è inferiore o uguale a 10, non scarta alcun Linguaggio, se è compresa tra 10 e 19, conserva solo gli \emph{n/2} Linguaggi a cui è associata la Confidence più elevata, se supera 20 vengono conservati solo gli \emph{n/4} Linguaggi con la Confidence più elevata.
    \item Il secondo motivo riguarda la presenza di Linguaggi nelle Repositories che pur essendo molto frequenti, non risultano rilevanti. Ad esempio, in molti Progetti sono presenti brevi script scritti in Bash utilizzati per motivi riguardanti la configurazione e l'esecuzione del Progetto; non sarebbe, dunque, corretto consigliare Bash unicamente perché molto usato insieme ad un determinato Linguaggio input. 
    Risulta, dunque, necessario svolgere un lavoro di tipo statistico su ogni Linguaggio candidato per determinare se, nelle Repositories in cui viene utilizzato insieme ad almeno uno dei Linguaggi input dell'utente, ha una percentuale di utilizzo almeno pari ad una determinata soglia. La percentuale di utilizzo di un Linguaggio in una Repository può essere facilmente ricavata a partire dalla quantità di byte di codice scritto in quel Linguaggio; ai fini dell'Algoritmo viene effettuata una media delle percentuali di codice scritto nel Linguaggio candidato considerato: verranno scartati tutti i Linguaggi la cui media di percentuale di codice impiegato nelle Repositories risulta essere inferiore al 3.5\% (tale soglia è stata scelta dopo aver effettuato diversi test e confrontato tra loro i risultati). 
\end{itemize}
\subsection{Suggerimento di Frameworks e Librerie: Natural Language Processing}
Suggerire Frameworks e Librerie ad un utente a partire dai Linguaggi, dai Frameworks e dalle Librerie che conosce, si è rivelato essere un lavoro meno agevole del precedente, in quanto le API di GitHub consentono di ricavare i Linguaggi adoperati in una Repository, ma non i Frameworks e le Librerie. È stata, dunque, adottata una tecnica di Intelligenza Artificiale diversa dalla precedente, al fine di sfruttare al meglio il DataSet in possesso; tale DataSet contiene:
\begin{itemize}
    \item Descrizioni in lingua inglese di Linguaggi, Framework e Librerie estratti tramite uno scraper scritto in \emph{JavaScript} dalla pagina di GitHub relativa ai Topic (tra cui Frameworks e Librerie) relativi alle Repositories. 
    \item Lista di Tag relativi a diverse Repositories estratti da GitHub tramite uno scraper appositamente scritto in \emph{JavaScript}. A differenza del caso dei Linguaggi, tuttavia, i tag sono inseriti dagli utenti e non sempre alla stessa Libreria/Framework corrisponde la stessa stringa, in quanto spesso vengono utilizzati nomi alternativi ed abbreviazioni (ciò non accade nel caso dei Linguaggi in quanto vengono automaticamente impostati da GitHub). 
\end{itemize}
Vista l'irregolarità e l'informalità del DataSet in esame, si è deciso di ricorrere a tecniche di \emph{Natural Language Processing} al fine di poter interpretare e dare un senso ai dati ed estrarre informazioni utili sulla correlazione tra i Frameworks, sulla correlazione tra le Librerie e sulla correlazione tra Frameworks e Librerie. L'Algoritmo di NLP utilizzato è \emph{Word2Vec} \cite{Word2Vec} che è basato su una Rete Neurale con un singolo livello nascosto il cui scopo è quello di calcolare un insieme di vettori al fine di valutare la similarità tra le parole facenti parte del DataSet. Quando si parla di \emph{Doc2Vec} è necessario dettagliare il concetto di DataSet mediante termini specifici: in primo luogo l'unità di base del DataSet è il \emph{documento}, definito come testo di qualsiasi tipo (una breve descrizione, un paper, un libro...) e che nel caso in questione risulta essere la descrizione di una Libreria/Framework oppure l'insieme di tag relativi ad una singola Repository. Un insieme di \emph{documenti} forma un \emph{Corpus}, che funge da input per l'Algoritmo e dal quale si estraggono le parole al fine di costruire un dizionario. Come anticipato, l'Algoritmo \emph{Word2Vec} fa uso di una rete neurale, nella quale è possibile individuare:
\begin{itemize}
    \item Un livello input che trasforma le parole in vettori: in primo luogo l'Algoritmo crea un dizionario contenente tutte le \emph{n} parole del \emph{Corpus} e alla parola i-esima associa un vettore di dimensione \emph{$1\times n$} composto da valori tutti pari a 0 e da un 1 in posizione i-esima; tali vettori sono chiamati \emph{one-hot vector}. 
    \item Un livello nascosto che contiene \emph{n} vettori, ciascuno di dimensione \emph{m}; la scelta di \emph{m} è, generalmente, problem specific e rappresenta il numero di \emph{features} delle parole nel dizionario considerato. Lo scopo della fase di addestramento è quello di apprendere i valori delle componenti dei vettori dello strato nascosto della rete, in quanto rappresentano le caratteristiche vere e proprie delle parole del dizionario; tali vettori sono denominati \emph{embeddings} e il livello nascosto della rete contiene una matrice \emph{$n \times m$} di tutti gli \emph{embeddings} relativi alle parole del dizionario.
    \item Un livello output nel quale viene effettuato il prodotto tra l'\emph{embedding} della parola input e la matrice che contiene tutti gli \emph{embeddings} trasposta, e successivamente al risultato viene applicata la funzione di regressione Softmax, ottenendo un vettore contenente la probabilità che ogni parola del dizionario si trovi nel contesto della parola input.
\end{itemize} 
L'addestramento della rete neurale consiste in diverse fasi: in primo luogo vi è la definizione di un valore intero detto \emph{window size} che rappresenta il numero di parole da considerare a sinistra e a destra di una parola \emph{w} all'interno di una frase del \emph{Corpus}. Tali parole saranno considerate appartenenti allo stesso contesto di \emph{w} e sono dette \emph{parole di contesto}. Vengono, a questo punto, costruiti dei campioni di addestramento ossia delle coppie \emph{(w, x)} dove \emph{w} è una parola del dizionario e \emph{x} è una sua parola di contesto; generate tutte queste coppie, viene data in input alla rete neurale la parola \emph{w}, ottenendo, in questo modo, un vettore output \emph{v} contenente le probabilità che ogni parola del dizionario si trovi nel contesto di \emph{w}. A questo punto si confronta il vettore \emph{v} con il vettore \emph{x} (che essendo un \emph{one-hot vector} conterrà un 1 nella posizione relativa alla parola di contesto) e si aggiornano gli \emph{embeddings} del livello nascosto della rete utilizzando il metodo della discesa del gradiente. Tale tipo di addestramento fa sì che i pesi del livello intermedio vadano a modificarsi gradualmente sulla base delle coppie di parole vicine che vengono prelevate dal \emph{Corpus}: al termine dell'addestramento, le parole che vengono considerate appartenenti allo stesso contesto, avranno i relativi \emph{embeddings} simili. Dal momento che un \emph{embedding} non è altro che un vettore, il risultato dell'addestramento può essere immaginato in uno spazio a \emph{m} dimensioni in cui ogni parola è rappresentata da un punto le cui coordinate sono descritte dal corrispondente \emph{embedding}, per cui parole usate in contesti simili, e quindi con \emph{embeddings} simili si tradurranno in punti molto vicini tra loro nello spazio a \emph{m} dimensioni. In questo modo, la Rete Neurale riesce ad apprendere il contesto in cui un Framework o una Libreria vengono usati e ad affiancare tra loro Frameworks e Librerie in base al contesto appreso.\\
L'Algoritmo appena descritto non è stato implementato da zero, ma è stata utilizzata una Libreria scritta in Python denominata \emph{Gensim} \cite{rehurek_lrec} che offre l'implementazione dell'Algoritmo \emph{Word2Vec} con la possibilità di impostare i parametri per l'addestramento della Rete Neurale; a tale scopo è stata impostata la dimensione dello spazio vettoriale a 200 e il numero di epoche della rete a 50. Prima di sottoporre il Corpus all'Algoritmo, è stata effettuata una fase di \emph{preprocessing} che ha rimosso le \emph{stopwords} dai \emph{documenti} del \emph{Corpus}, in particolare dalle descrizioni dei Frameworks e delle Librerie. Il lavoro descritto fino a questo momento viene svolto \emph{una tantum} in quanto funge da addestramento per la rete neurale, al seguito del quale viene salvato il modello addestrato in maniera persistente. 
Una volta salvato il modello addestrato, è possibile testarlo su diversi input al fine di stabilire le parole più simili, in termini di contesto, alla parola input. Ponendo nuovamente l'attenzione sulla piattaforma \textsc{Code4Code}, la parola input sarà un Linguaggio, un Framework o una Libreria, così come dalle parole output verranno selezionati solo i Frameworks e le Librerie. In particolare verranno selezionati solo quelli la cui probabilità di trovarsi nel contesto della parola input è superiore al 50\%. L'utente, dunque, seleziona i Linguaggi e i Framework che conosce, e la Rete Neurale processerà ognuna di queste Tecnologie per produrre un insieme di Frameworks e Librerie che gli consiglierà.
\newpage
